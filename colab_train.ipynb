{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfdab55",
   "metadata": {},
   "source": [
    "# Aetheria — Colab Training Notebook\n",
    "\n",
    "This notebook handles the **heavy** parts of the Original Sin pipeline:\n",
    "- **Gluttony** — distil conversations from DialoGPT\n",
    "- **Train** — train TinyTransformerLM on all collected data\n",
    "- **Download** — saves the `.pt` checkpoint so you can copy it back to `models/`\n",
    "\n",
    "**Runtime → Change runtime type → T4 GPU** before running.\n",
    "\n",
    "---\n",
    "Upload the `Aetheria/` folder to your Google Drive under `MyDrive/AetheriaAI/Aetheria/`,\n",
    "or use the zip upload cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21edbb7",
   "metadata": {},
   "source": [
    "## Step 1 — Mount Google Drive & set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# ── Edit this if your Drive path is different ──────────────────────────────\n",
    "AETHERIA_ROOT = '/content/drive/MyDrive/AetheriaAI/Aetheria'\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "os.chdir(AETHERIA_ROOT)\n",
    "print('Working directory:', os.getcwd())\n",
    "print('Files:', os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdec33b",
   "metadata": {},
   "source": [
    "### Alternative — Upload a zip instead of using Drive\n",
    "If you prefer to upload a zip of your `Aetheria/` folder, run the next cell.\n",
    "Skip it if you already mounted Drive above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP THIS CELL if you used Drive above\n",
    "# from google.colab import files\n",
    "# import zipfile, os\n",
    "#\n",
    "# uploaded = files.upload()          # upload Aetheria.zip\n",
    "# zip_name = list(uploaded.keys())[0]\n",
    "# with zipfile.ZipFile(zip_name, 'r') as z:\n",
    "#     z.extractall('/content/')\n",
    "# AETHERIA_ROOT = '/content/Aetheria'\n",
    "# os.chdir(AETHERIA_ROOT)\n",
    "# print('Extracted to', AETHERIA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba4a14",
   "metadata": {},
   "source": [
    "## Step 2 — Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec26294",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece transformers accelerate\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0114a64",
   "metadata": {},
   "source": [
    "## Step 3 — Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('WARNING: No GPU found. Go to Runtime → Change runtime type → T4 GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383fa779",
   "metadata": {},
   "source": [
    "## Step 4 — Gluttony: devour TinyLlama\n",
    "\n",
    "**Two modes run back to back:**\n",
    "1. `generate` — TinyLlama answers seed prompts → saved as text training pairs\n",
    "2. `distill` — TRUE distillation: student trains on TinyLlama's full probability distributions via KL loss → saved as `models/aetheria_distilled.pt`\n",
    "\n",
    "TinyLlama (~2.2 GB) downloads on first run. T4 GPU handles both in ~10 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ── 4a: generate text pairs (feeds into clean → train pipeline) ──────────────\n",
    "!python Original_sin/gluttony/gluttony.py \\\n",
    "    --model tinyllama \\\n",
    "    --mode generate \\\n",
    "    --rounds 100 \\\n",
    "    --output data/gluttony_conversations.txt\n",
    "\n",
    "size = os.path.getsize('data/gluttony_conversations.txt') if os.path.exists('data/gluttony_conversations.txt') else 0\n",
    "print(f'gluttony_conversations.txt: {size/1024:.1f} KB')\n",
    "\n",
    "# ── 4b: true KL distillation → aetheria_distilled.pt ────────────────────────\n",
    "!python Original_sin/gluttony/gluttony.py \\\n",
    "    --model tinyllama \\\n",
    "    --mode distill \\\n",
    "    --epochs 5 \\\n",
    "    --temperature 3.0 \\\n",
    "    --alpha 0.7\n",
    "\n",
    "size2 = os.path.getsize('models/aetheria_distilled.pt') if os.path.exists('models/aetheria_distilled.pt') else 0\n",
    "print(f'aetheria_distilled.pt: {size2/1024/1024:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba5508",
   "metadata": {},
   "source": [
    "## Step 5 — Merge and clean all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e991c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge envy + gluttony data into conversations.txt\n",
    "raw_path = 'data/conversations.txt'\n",
    "sources = ['data/envy_conversations.txt', 'data/gluttony_conversations.txt']\n",
    "\n",
    "with open(raw_path, 'a', encoding='utf-8') as out:\n",
    "    for src in sources:\n",
    "        if os.path.exists(src):\n",
    "            text = open(src, encoding='utf-8', errors='ignore').read()\n",
    "            out.write(text + '\\n')\n",
    "            print(f'  merged: {src}')\n",
    "\n",
    "!python scripts/clean_data.py\n",
    "\n",
    "size = os.path.getsize('data/cleaned_conversations.txt')\n",
    "print(f'cleaned_conversations.txt: {size/1024:.1f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e70ae6",
   "metadata": {},
   "source": [
    "## Step 6 — Train SentencePiece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41726edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/train_spm.py \\\n",
    "    --input data/cleaned_conversations.txt \\\n",
    "    --model_prefix data/spm \\\n",
    "    --vocab_size 8000\n",
    "\n",
    "print('SPM model:', os.path.exists('data/spm.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f4dc1",
   "metadata": {},
   "source": [
    "## Step 7 — Train TinyTransformerLM\n",
    "\n",
    "Adjust `--epochs` and `--batch_size` to taste.  \n",
    "With a T4 GPU, 20 epochs on ~1 MB of text takes ~10-20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/prototype_model.py \\\n",
    "    --data data/cleaned_conversations.txt \\\n",
    "    --spm  data/spm.model \\\n",
    "    --vocab_size 8000 \\\n",
    "    --epochs 20 \\\n",
    "    --batch_size 32 \\\n",
    "    --seq_len 128 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c91417",
   "metadata": {},
   "source": [
    "## Step 8 — Copy checkpoint back to Drive & download\n",
    "\n",
    "The `.pt` file is saved inside `models/`. Copy to Drive root so you can grab it easily,\n",
    "then also offer a direct browser download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, glob\n",
    "\n",
    "pts = sorted(glob.glob('models/*.pt'))\n",
    "if not pts:\n",
    "    print('No checkpoint found. Did training finish?')\n",
    "else:\n",
    "    latest = pts[-1]\n",
    "    size_mb = os.path.getsize(latest) / (1024*1024)\n",
    "    print(f'Found checkpoint: {latest}  ({size_mb:.1f} MB)')\n",
    "\n",
    "    # Copy to Drive root for easy access\n",
    "    drive_dest = '/content/drive/MyDrive/aetheria_latest.pt'\n",
    "    shutil.copy(latest, drive_dest)\n",
    "    print(f'Copied to Drive: {drive_dest}')\n",
    "\n",
    "    # Also copy tokenizer files\n",
    "    for tok_file in ['data/spm.model', 'data/spm.vocab']:\n",
    "        if os.path.exists(tok_file):\n",
    "            dest = f'/content/drive/MyDrive/{os.path.basename(tok_file)}'\n",
    "            shutil.copy(tok_file, dest)\n",
    "            print(f'Copied tokenizer: {dest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct browser download\n",
    "from google.colab import files\n",
    "\n",
    "# Download trained model\n",
    "pts = sorted(glob.glob('models/*.pt'))\n",
    "for pt in pts:\n",
    "    files.download(pt)\n",
    "    print(f'Downloading: {pt}')\n",
    "\n",
    "# Download tokenizer\n",
    "for f in ['data/spm.model', 'data/spm.vocab']:\n",
    "    if os.path.exists(f):\n",
    "        files.download(f)\n",
    "        print(f'Downloading: {f}')\n",
    "\n",
    "print('\\nPlace files locally:')\n",
    "print('  models/aetheria_colab.pt')\n",
    "print('  models/aetheria_distilled.pt')\n",
    "print('  data/spm.model  (replace existing)')\n",
    "print('  data/spm.vocab  (replace existing)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e91571",
   "metadata": {},
   "source": [
    "---\n",
    "## Done!\n",
    "\n",
    "Once downloaded, place the files in your local project:\n",
    "```\n",
    "Aetheria/models/aetheria_latest.pt   ← rename if needed\n",
    "Aetheria/data/spm.model\n",
    "Aetheria/data/spm.vocab\n",
    "```\n",
    "Then talk to Aetheria locally:\n",
    "```powershell\n",
    "python Original_sin/aetheria_core.py talk\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
