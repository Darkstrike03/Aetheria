{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d17c999d",
   "metadata": {},
   "source": [
    "# Aetheria O1 — Full Training Pipeline\n",
    "\n",
    "This notebook runs the complete **Original Sin O1** pipeline:\n",
    "\n",
    "| Step | What happens | Output |\n",
    "|------|-------------|--------|\n",
    "| 1 | Mount Drive + env setup | — |\n",
    "| 2 | Install deps | — |\n",
    "| 3 | GPU check | — |\n",
    "| 4 | **Envy** — calls Groq LLM, generates seed conversations | `data/envy_conv.txt` |\n",
    "| 5 | **Gluttony** — TinyLlama generates training pairs | `data/gluttony_conv.txt` |\n",
    "| 6 | Clean both conv files | `data/envy_cleaned.txt`, `data/gluttony_cleaned.txt` |\n",
    "| 7 | Train **SPM tokenizer** on combined data | `data/spm.model`, `data/spm.vocab` |\n",
    "| 8 | Train model on envy data | `models/envy.pt` |\n",
    "| 9 | Train model on gluttony data | `models/gluttony.pt` |\n",
    "| 10 | **Merge** envy.pt + gluttony.pt (50/50) | `models/aetheria_o1.pt` |\n",
    "| 11 | Download all artefacts | browser download |\n",
    "\n",
    "**Runtime → Change runtime type → T4 GPU** before running.\n",
    "\n",
    "Upload `Aetheria/` to `MyDrive/AetheriaAI/Aetheria/` in Google Drive first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16844f32",
   "metadata": {},
   "source": [
    "## Step 1 — Mount Google Drive & set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf826ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# ── Edit this if your Drive path is different ──────────────────────\n",
    "AETHERIA_ROOT = '/content/drive/MyDrive/AetheriaAI/Aetheria'\n",
    "# ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "os.chdir(AETHERIA_ROOT)\n",
    "print('Working directory:', os.getcwd())\n",
    "print('Files:', os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676a2c6",
   "metadata": {},
   "source": [
    "## Step 2 — Install dependencies & set API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0282682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-install sympy so torch.distributed doesn't stall on it during transformers import\n",
    "!pip install -q sympy\n",
    "\n",
    "# Install remaining deps (no version pin — let Colab resolve normally)\n",
    "!pip install -q sentencepiece transformers accelerate requests\n",
    "\n",
    "# Verify transformers imports cleanly\n",
    "import transformers\n",
    "print(f'transformers {transformers.__version__} ✓')\n",
    "\n",
    "# ── Set your Groq API key (needed for Envy) ────────────────────────\n",
    "# Get a free key at https://console.groq.com\n",
    "import os\n",
    "os.environ['GROQ_API_KEY'] = ''   # ← paste your key here\n",
    "\n",
    "# Or load from .env file already in your Drive folder:\n",
    "# from dotenv import load_dotenv; load_dotenv('.env')\n",
    "\n",
    "if os.environ.get('GROQ_API_KEY'):\n",
    "    print('Groq key: set ✓')\n",
    "else:\n",
    "    print('WARNING: GROQ_API_KEY not set — Envy will fail. Set it above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda17344",
   "metadata": {},
   "source": [
    "## Step 3 — Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d284bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('WARNING: No GPU. Go to Runtime → Change runtime type → T4 GPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff863a",
   "metadata": {},
   "source": [
    "## Step 4 — Envy: call Groq → `data/envy_conv.txt`\n",
    "\n",
    "Envy asks the free Groq LLM seed questions in Aetheria's style.  \n",
    "Increase `--rounds` for more data (each round = 1 API call, ~1.5s delay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b633f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVY_ROUNDS = 80   # ← adjust: more rounds = more data\n",
    "\n",
    "!python Original_sin/envy/envy.py \\\n",
    "    --provider groq \\\n",
    "    --rounds {ENVY_ROUNDS} \\\n",
    "    --output data/envy_conv.txt\n",
    "\n",
    "import os\n",
    "size = os.path.getsize('data/envy_conv.txt') if os.path.exists('data/envy_conv.txt') else 0\n",
    "print(f'envy_conv.txt: {size/1024:.1f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c94991",
   "metadata": {},
   "source": [
    "## Step 5 — Gluttony: devour TinyLlama → `data/gluttony_conv.txt`\n",
    "\n",
    "Gluttony feeds seed prompts to TinyLlama and records (prompt, response) pairs.  \n",
    "TinyLlama (~2.2 GB) downloads on first run. T4 handles 100 rounds in ~5 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e69e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUTTONY_ROUNDS = 100   # ← adjust\n",
    "\n",
    "!python Original_sin/gluttony/gluttony.py \\\n",
    "    --model tinyllama \\\n",
    "    --mode generate \\\n",
    "    --rounds {GLUTTONY_ROUNDS} \\\n",
    "    --output data/gluttony_conv.txt\n",
    "\n",
    "size = os.path.getsize('data/gluttony_conv.txt') if os.path.exists('data/gluttony_conv.txt') else 0\n",
    "print(f'gluttony_conv.txt: {size/1024:.1f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f152c",
   "metadata": {},
   "source": [
    "## Step 6 — Clean both conv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "sys.path.insert(0, 'scripts')\n",
    "from clean_data import clean_text\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_file(src: str, dst: str):\n",
    "    raw = Path(src).read_text(encoding='utf-8', errors='ignore')\n",
    "    paras = clean_text(raw)\n",
    "    Path(dst).write_text('\\n\\n'.join(paras), encoding='utf-8')\n",
    "    print(f'{src} → {dst}  ({len(paras)} paragraphs, {Path(dst).stat().st_size/1024:.1f} KB)')\n",
    "\n",
    "clean_file('data/envy_conv.txt',     'data/envy_cleaned.txt')\n",
    "clean_file('data/gluttony_conv.txt', 'data/gluttony_cleaned.txt')\n",
    "\n",
    "# Also write a combined file for SPM training\n",
    "combined = Path('data/envy_cleaned.txt').read_text(encoding='utf-8') + '\\n\\n' + \\\n",
    "           Path('data/gluttony_cleaned.txt').read_text(encoding='utf-8')\n",
    "Path('data/o1_combined.txt').write_text(combined, encoding='utf-8')\n",
    "print(f'o1_combined.txt: {len(combined)/1024:.1f} KB (used for tokenizer training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134cc5b2",
   "metadata": {},
   "source": [
    "## Step 7 — Train SentencePiece tokenizer on combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/train_spm.py \\\n",
    "    --input data/o1_combined.txt \\\n",
    "    --model_prefix data/spm \\\n",
    "    --vocab_size 8000\n",
    "\n",
    "print('spm.model exists:', os.path.exists('data/spm.model'))\n",
    "print('spm.vocab exists:', os.path.exists('data/spm.vocab'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dca765",
   "metadata": {},
   "source": [
    "## Step 8 — Train on Envy data → `models/envy.pt`\n",
    "\n",
    "This model learns the voice/style that came from the Groq LLM (Envy's harvest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVY_EPOCHS = 20   # ← adjust\n",
    "\n",
    "!python scripts/prototype_model.py \\\n",
    "    --data    data/envy_cleaned.txt \\\n",
    "    --spm     data/spm.model \\\n",
    "    --vocab_size 8000 \\\n",
    "    --epochs  {ENVY_EPOCHS} \\\n",
    "    --batch_size 32 \\\n",
    "    --seq_len 128 \\\n",
    "    --device  cuda \\\n",
    "    --ckpt_name envy.pt\n",
    "\n",
    "size = os.path.getsize('models/envy.pt') if os.path.exists('models/envy.pt') else 0\n",
    "print(f'envy.pt: {size/1024/1024:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e278a0",
   "metadata": {},
   "source": [
    "## Step 9 — Train on Gluttony data → `models/gluttony.pt`\n",
    "\n",
    "This model learns from what TinyLlama generated (Gluttony's harvest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eaabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUTTONY_EPOCHS = 20   # ← adjust\n",
    "\n",
    "!python scripts/prototype_model.py \\\n",
    "    --data    data/gluttony_cleaned.txt \\\n",
    "    --spm     data/spm.model \\\n",
    "    --vocab_size 8000 \\\n",
    "    --epochs  {GLUTTONY_EPOCHS} \\\n",
    "    --batch_size 32 \\\n",
    "    --seq_len 128 \\\n",
    "    --device  cuda \\\n",
    "    --ckpt_name gluttony.pt\n",
    "\n",
    "size = os.path.getsize('models/gluttony.pt') if os.path.exists('models/gluttony.pt') else 0\n",
    "print(f'gluttony.pt: {size/1024/1024:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4614a23f",
   "metadata": {},
   "source": [
    "## Step 10 — Merge envy.pt + gluttony.pt → `models/aetheria_o1.pt`\n",
    "\n",
    "Weight averaging (model soup): blends both checkpoints into one.\n",
    "Adjust `--weight_a` to weight envy more (0.7) or gluttony more (0.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/merge_models.py \\\n",
    "    --a        models/envy.pt \\\n",
    "    --b        models/gluttony.pt \\\n",
    "    --out      models/aetheria_o1.pt \\\n",
    "    --weight_a 0.5\n",
    "\n",
    "size = os.path.getsize('models/aetheria_o1.pt') if os.path.exists('models/aetheria_o1.pt') else 0\n",
    "print(f'aetheria_o1.pt: {size/1024/1024:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683dbd3",
   "metadata": {},
   "source": [
    "## Step 11 — Copy to Drive & download everything\n",
    "\n",
    "Files to download and place locally:\n",
    "```\n",
    "models/aetheria_o1.pt   → Aetheria/models/aetheria_o1.pt\n",
    "models/envy.pt          → Aetheria/models/envy.pt       (keep for reference)\n",
    "models/gluttony.pt      → Aetheria/models/gluttony.pt   (keep for reference)\n",
    "data/spm.model          → Aetheria/data/spm.model\n",
    "data/spm.vocab          → Aetheria/data/spm.vocab\n",
    "```\n",
    "\n",
    "Then talk to Aetheria:\n",
    "```powershell\n",
    "python Original_sin/aetheria_core.py talk --ckpt models\\aetheria_o1.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "DRIVE_DEST = '/content/drive/MyDrive/AetheriaAI_output'\n",
    "os.makedirs(DRIVE_DEST, exist_ok=True)\n",
    "\n",
    "artefacts = [\n",
    "    'models/aetheria_o1.pt',\n",
    "    'models/envy.pt',\n",
    "    'models/gluttony.pt',\n",
    "    'data/spm.model',\n",
    "    'data/spm.vocab',\n",
    "    'data/envy_conv.txt',\n",
    "    'data/gluttony_conv.txt',\n",
    "]\n",
    "\n",
    "for f in artefacts:\n",
    "    if os.path.exists(f):\n",
    "        dest = os.path.join(DRIVE_DEST, os.path.basename(f))\n",
    "        shutil.copy(f, dest)\n",
    "        print(f'  ✓ {f}  →  {dest}')\n",
    "    else:\n",
    "        print(f'  ✗ {f}  (not found)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct browser download (optional — Drive copy above is usually enough)\n",
    "from google.colab import files\n",
    "\n",
    "for f in ['models/aetheria_o1.pt', 'data/spm.model', 'data/spm.vocab']:\n",
    "    if os.path.exists(f):\n",
    "        files.download(f)\n",
    "        print(f'Downloading {f}...')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
