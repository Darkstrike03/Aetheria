{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88572c96",
   "metadata": {},
   "source": [
    "# Aetheria — Feedback Retrain Notebook\n",
    "\n",
    "**Purpose:** Take `feedback.jsonl` you collected locally with Envy teach mode,\n",
    "and fine-tune `aetheria_soul.pt` on a GPU so the training is fast and deep.\n",
    "\n",
    "**Workflow:**\n",
    "1. Collect feedback locally: `python Original_sin/envy/envy.py --mode teach`\n",
    "2. Push to GitHub: `git add data/feedback.jsonl models/aetheria_soul.pt && git commit && git push`\n",
    "3. Run this notebook on Colab (T4 GPU)\n",
    "4. Download the new `.pt` → replace your local `models/aetheria_soul.pt`\n",
    "\n",
    "**Before running:** `Runtime → Change runtime type → T4 GPU`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35a9cb",
   "metadata": {},
   "source": [
    "## Step 1 — Clone repo from GitHub\n",
    "Change `GITHUB_REPO` to your repo URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Edit this to your GitHub repo ─────────────────────────────────────────\n",
    "GITHUB_REPO = 'https://github.com/YOUR_USERNAME/AetheriaAI.git'\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('/content/AetheriaAI'):\n",
    "    !git clone {GITHUB_REPO} /content/AetheriaAI\n",
    "else:\n",
    "    !cd /content/AetheriaAI && git pull\n",
    "\n",
    "AETHERIA_ROOT = '/content/AetheriaAI/Aetheria'\n",
    "os.chdir(AETHERIA_ROOT)\n",
    "print('Working directory:', os.getcwd())\n",
    "print('Files:', os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46270ac",
   "metadata": {},
   "source": [
    "### Alternative — Mount Google Drive instead\n",
    "If you keep your project on Drive, run this cell instead of the GitHub one above.\n",
    "Skip if you used GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb347d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP if you used GitHub above\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import os\n",
    "# AETHERIA_ROOT = '/content/drive/MyDrive/AetheriaAI/Aetheria'\n",
    "# os.chdir(AETHERIA_ROOT)\n",
    "# print('Working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70b2dc",
   "metadata": {},
   "source": [
    "## Step 2 — Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b6a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece transformers accelerate\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae1f0a0",
   "metadata": {},
   "source": [
    "## Step 3 — Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae796b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('WARNING: No GPU. Go to Runtime → Change runtime type → T4 GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b2b96",
   "metadata": {},
   "source": [
    "## Step 4 — Check feedback.jsonl and checkpoint\n",
    "\n",
    "If using GitHub, this should already be there after `git pull`.\n",
    "If your feedback isn't in the repo, use the upload cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719145b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "feedback_path = 'data/feedback.jsonl'\n",
    "ckpt_path     = 'models/aetheria_soul.pt'\n",
    "spm_path      = 'data/spm.model'\n",
    "\n",
    "# Count feedback pairs\n",
    "if os.path.exists(feedback_path):\n",
    "    pairs = [json.loads(l) for l in open(feedback_path, encoding='utf-8') if l.strip()]\n",
    "    print(f'feedback.jsonl : {len(pairs)} pairs')\n",
    "    print(f'  Sample: Q={pairs[0][\"human\"][:50]}  A={pairs[0][\"aetheria\"][:50]}')\n",
    "else:\n",
    "    print('feedback.jsonl NOT FOUND — upload it below')\n",
    "\n",
    "# Check checkpoint\n",
    "if os.path.exists(ckpt_path):\n",
    "    size_mb = os.path.getsize(ckpt_path) / (1024*1024)\n",
    "    print(f'Checkpoint     : {ckpt_path}  ({size_mb:.1f} MB)')\n",
    "else:\n",
    "    print('aetheria_soul.pt NOT FOUND — upload it below')\n",
    "\n",
    "# Check SPM tokenizer\n",
    "print(f'spm.model      : {\"OK\" if os.path.exists(spm_path) else \"NOT FOUND\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1717d",
   "metadata": {},
   "source": [
    "### Manual upload (only if files are missing after git pull)\n",
    "Run this to upload `feedback.jsonl`, `aetheria_soul.pt`, and `spm.model` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP if all files were found above\n",
    "# from google.colab import files as colab_files\n",
    "# import shutil, os\n",
    "#\n",
    "# print('Upload: feedback.jsonl, aetheria_soul.pt, spm.model, spm.vocab')\n",
    "# uploaded = colab_files.upload()\n",
    "# for fname, data in uploaded.items():\n",
    "#     if 'feedback' in fname:\n",
    "#         dest = 'data/feedback.jsonl'\n",
    "#     elif fname.endswith('.pt'):\n",
    "#         dest = 'models/' + fname\n",
    "#     elif 'spm.model' in fname:\n",
    "#         dest = 'data/spm.model'\n",
    "#     elif 'spm.vocab' in fname:\n",
    "#         dest = 'data/spm.vocab'\n",
    "#     else:\n",
    "#         dest = fname\n",
    "#     os.makedirs(os.path.dirname(dest) or '.', exist_ok=True)\n",
    "#     with open(dest, 'wb') as f:\n",
    "#         f.write(data)\n",
    "#     print(f'Saved: {dest}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc24b16",
   "metadata": {},
   "source": [
    "## Step 5 — Retrain on feedback\n",
    "\n",
    "Fine-tunes `aetheria_soul.pt` on your approved pairs.\n",
    "\n",
    "**Tuning guide:**\n",
    "- `EPOCHS`: 10–20 for <100 pairs, 5–10 for 200+ pairs\n",
    "- `LR`: `3e-5` is safe; go up to `1e-4` if loss barely moves\n",
    "- `BATCH_SIZE`: 16–32 on T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41516f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Tune these ────────────────────────────────────────────────────────────\n",
    "EPOCHS      = 15\n",
    "LR          = 3e-5\n",
    "BATCH_SIZE  = 16\n",
    "SEQ_LEN     = 64\n",
    "CKPT_IN     = 'models/aetheria_soul.pt'\n",
    "CKPT_OUT    = 'models/aetheria_soul.pt'   # overwrite base (safe — git has the original)\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "!python scripts/retrain_feedback.py \\\n",
    "    --ckpt   {CKPT_IN} \\\n",
    "    --out    {CKPT_OUT} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --lr     {LR} \\\n",
    "    --batch_size {BATCH_SIZE} \\\n",
    "    --seq_len    {SEQ_LEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678d9aa",
   "metadata": {},
   "source": [
    "## Step 6 — (Optional) Blind Devour on top\n",
    "\n",
    "Run this **after** feedback retrain to also absorb DialoGPT-small's conversational\n",
    "flow. Skip if you just want the feedback-only model.\n",
    "\n",
    "Takes ~15 min on T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL — skip if you only want feedback retrain\n",
    "# !python Original_sin/aetheria_core.py gluttony \\\n",
    "#     --gluttony_mode blind_devour \\\n",
    "#     --gluttony_model dialogpt-small \\\n",
    "#     --ckpt models/aetheria_soul.pt \\\n",
    "#     --epochs 3 \\\n",
    "#     --conversations 30 \\\n",
    "#     --turns 6 \\\n",
    "#     --pride_weight 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f57af",
   "metadata": {},
   "source": [
    "## Step 7 — Quick sanity test\n",
    "\n",
    "Run a few prompts through the model to see if it's coherent before downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, math\n",
    "import torch.nn as nn\n",
    "import sentencepiece as spm\n",
    "\n",
    "sys.path.insert(0, 'scripts')\n",
    "\n",
    "# ── Minimal inline model (mirrors lust.py) ────────────────────────────────\n",
    "class _PE(nn.Module):\n",
    "    def __init__(self, d, mx=1024):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(mx, d)\n",
    "        pos = torch.arange(0, mx, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d, 2).float() * (-math.log(10000.0) / d))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, v, d=256, h=4, L=4, ff=1024):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(v, d)\n",
    "        self.pe  = _PE(d)\n",
    "        enc = nn.TransformerEncoderLayer(d, h, ff, batch_first=True)\n",
    "        self.tr = nn.TransformerEncoder(enc, L)\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "        self.head = nn.Linear(d, v)\n",
    "    def forward(self, x):\n",
    "        e = self.pe(self.emb(x))\n",
    "        return self.head(self.ln(self.tr(e.transpose(0,1)).transpose(0,1)))\n",
    "\n",
    "def generate(model, tok, prompt, max_new=80, top_p=0.85, temp=0.75, rep=1.5):\n",
    "    v = len(tok)\n",
    "    ids = tok.encode(prompt)[-512:]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new):\n",
    "            x = torch.tensor([ids])\n",
    "            logits = model(x)[0, -1]\n",
    "            if logits.size(0) > v: logits[v:] = float('-inf')\n",
    "            # rep penalty\n",
    "            for g in set(ids[-64:]):\n",
    "                if 0 <= g < logits.size(0):\n",
    "                    logits[g] = logits[g]/rep if logits[g]>0 else logits[g]*rep\n",
    "            logits = logits / temp\n",
    "            probs = torch.softmax(logits, -1)\n",
    "            sp, si = torch.sort(probs, descending=True)\n",
    "            cum = torch.cumsum(sp, 0)\n",
    "            keep = (cum - sp) < top_p; keep[0] = True\n",
    "            mask = torch.zeros_like(probs, dtype=torch.bool)\n",
    "            mask[si[keep]] = True\n",
    "            probs = probs * mask; probs = probs / probs.sum()\n",
    "            nxt = int(torch.multinomial(probs, 1).item())\n",
    "            ids.append(max(0, min(nxt, v-1)))\n",
    "    return tok.decode([max(0, min(i, v-1)) for i in ids])\n",
    "\n",
    "def reply(model, tok, question):\n",
    "    prompt = f'Human: {question}\\nAetheria:'\n",
    "    full   = generate(model, tok, prompt)\n",
    "    text   = full[len(prompt):].strip()\n",
    "    if 'Human:' in text: text = text.split('Human:')[0].strip()\n",
    "    return text\n",
    "\n",
    "# ── Load ──────────────────────────────────────────────────────────────────\n",
    "ckpt = torch.load('models/aetheria_soul.pt', map_location='cpu', weights_only=False)\n",
    "vs   = ckpt.get('vocab_size', 0)\n",
    "st   = ckpt.get('model_state_dict', ckpt)\n",
    "if vs <= 0:\n",
    "    for k in ('token_emb.weight', 'embedding.weight'):\n",
    "        if k in st: vs = st[k].shape[0]; break\n",
    "\n",
    "tok = spm.SentencePieceProcessor()\n",
    "tok.Load('data/spm.model')\n",
    "\n",
    "m = TinyLM(vs)\n",
    "ns = m.state_dict()\n",
    "for k, v in ns.items():\n",
    "    if k in st:\n",
    "        vc = st[k]\n",
    "        if vc.shape == v.shape: ns[k] = vc\n",
    "        elif k.endswith(('emb.weight','head.weight','head.bias')):\n",
    "            n = min(vc.shape[0], v.shape[0]); ns[k][:n] = vc[:n]\n",
    "m.load_state_dict(ns)\n",
    "\n",
    "# ── V1 readiness tests ────────────────────────────────────────────────────\n",
    "TEST_PROMPTS = [\n",
    "    'hello',\n",
    "    'who are you?',\n",
    "    'are you god?',\n",
    "    'how do you do?',\n",
    "    'what are you made of?',\n",
    "    'do you feel emotions?',\n",
    "]\n",
    "\n",
    "print('='*60)\n",
    "print('  V1 READINESS TEST')\n",
    "print('='*60)\n",
    "for q in TEST_PROMPTS:\n",
    "    r = reply(m, tok, q)\n",
    "    print(f'  Q: {q}')\n",
    "    print(f'  A: {r}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd8624",
   "metadata": {},
   "source": [
    "## Step 8 — Push updated model back to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the retrained checkpoint back to your repo\n",
    "# Fill in your GitHub email + name if git complains\n",
    "\n",
    "!git config user.email 'you@example.com'\n",
    "!git config user.name  'Aetheria Colab'\n",
    "\n",
    "!git add models/aetheria_soul.pt\n",
    "!git commit -m 'Colab retrain: feedback baked in'\n",
    "\n",
    "# For private repos you need a personal access token:\n",
    "# !git remote set-url origin https://YOUR_TOKEN@github.com/YOUR_USERNAME/AetheriaAI.git\n",
    "\n",
    "!git push\n",
    "print('Pushed. Pull locally with: git pull')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0908843",
   "metadata": {},
   "source": [
    "### Alternative — Download directly to your browser\n",
    "If you don't use GitHub, run this to download the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP if you used git push above\n",
    "# from google.colab import files as colab_files\n",
    "# colab_files.download('models/aetheria_soul.pt')\n",
    "# print('Downloaded. Place at: Aetheria/models/aetheria_soul.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcd049",
   "metadata": {},
   "source": [
    "---\n",
    "## Done!\n",
    "\n",
    "**Local next steps:**\n",
    "```powershell\n",
    "git pull                         # get the retrained model\n",
    "python Original_sin/aetheria_core.py talk --ckpt models/aetheria_soul.pt\n",
    "```\n",
    "\n",
    "**Loop:**\n",
    "```\n",
    "Local: python Original_sin/envy/envy.py --mode teach   (add 20-40 pairs)\n",
    "       git add data/feedback.jsonl && git commit && git push\n",
    "Colab: git pull → run Step 5 → git push\n",
    "Local: git pull → talk\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
